Deploying ubuntu-autopkgtest-operators
===========================

ubuntu-autopkgtest-operators is designed around the requirements of the production
deployment, and requires/assumes the following:

* A cloud to run the infrastructure itself on. Any cloud supported by Juju
  will do.
* LXD remotes to run tests on.
* A Swift instance to upload results to.

The supported deployment method is via `Terraform <https://developer.hashicorp.com/terraform>`_
and `Juju <https://jaas.ai/>`_. The Juju charms themselves need to be built
using the `charm` tool. The deployment scripts rely on `jq`. So you need the
following:

* ``snap:terraform``
* ``snap:juju``

Set up Terraform environment
-----------------------

First, set up Juju and Terraform. You need to have cloned the `terraform
plan <https://github.com/canonical/ubuntu-engineering-terraform-models>` for the orchestration environment.

.. code-block::

  juju bootstrap # if necessary - follow any instructions, using LXD is fine for a local deployment

  git clone https://github.com/canonical/ubuntu-engineering-terraform-models
  cd ubuntu-engineering-terraform-models/models/ps7/ubuntu-engineering/autopkgtest/prod-autopkgtest-orchestrator-ps7
  terraform plan -var local_run=true
  terraform apply -var local_run=true

at this point ``juju status`` should show the three deployed charms along with a subordinate
``ntp`` charm each.


Configure
---------

Now add each of the LXD remotes you would like to use as autopkgtest workers. If you are using the
``lxd-remote`` charm, you can use the dedicated action::
  $ juju run lxd-remote/leader get-client-token name=dispatcher
  $ juju run lxd-remote/leader get-client-token name=janitor

Otherwise, you can generate the tokens manually from within the machine you want to use as your LXD remote.

Use the tokens generated by the previous steps with the charms in the orchestration environment::
  $ juju run dispatcher/leader add-remote arch=<remote_arch> token=<dispatcher_token>
  $ juju run janitor/leader add-remote arch=<remote_arch> token=<dispatcher_token>

Once all remotes are configured you can check the resulting worker config::
  $ juju run dispatcher/leader show-target-config

Which should show you the target number of systemd units that will be spawned per worker. You can
change this number with::
  $ juju run dispatcher/leader set-worker-count arch=<worker_arch> count=<unit_count>

The default number of workers is stored in the config value ``default-worker-count``. Any changes
to this value will affect remotes added afterwards.

Deploy
------

Once you are happy with the unit count, you can create or destroy the systemd units as appropriate with::
  $ juju run dispatcher/leader reconcile-worker-units

The website charm should start the frontend automatically.

Making configuration changes
----------------------------

Edit the terraform plan and run ``terraform plan`` and ``terraform apply``.

About cloud environments quotas
-------------------------------

Each OpenStack environment has a quota, meaning there is a limit to the number
of instances, cpu cores, RAM, disk, etc, amount that can be spawned at the same
time.

Update the code
---------------

Note: see :ref:`testing wip changes` if you're pushing a work in progress
change.

The above reconfiguration only effects configuration changes. If you want to
change the charms themselves, you need to *build*, *upload to the charm store* and
then run a *refresh*. For example, for ``autopkgtest-dispatcher-operator``:

.. code-block::

 $ # this is all happening on your local development system
 $ charmcraft clean
 $ charmcraft pack
 [...]
 Charms packed:
    autopkgtest-dispatcher_amd64.charm
 $ charmcraft upload autopkgtest-dispatcher_amd64.charm --name ubuntu-autopkgtest-dispatcher
 Revision XX of 'ubuntu-autopkgtest-dispatcher' created
 $ # For staging use the edge channel
 $ # For production use the release channel
 $ charmcraft release ubuntu-autopkgtest-dispatcher --revision=XX --channel=$channel # using the revision number given above
 Revision XX of charm 'ubuntu-autopkgtest-dispatcher' released to edge
 $ # Test charm in staging
 $ # pack, upload and release to stable

IS GitOps will periodically refresh the deployed charms.


Using the staging environment
-----------------------------

If you've got access to the production deployment then there are a set of staging environments
available on the Ubuntu engineering bastion.

* The URL is `<https://autopkgtest.staging.ubuntu.com>`_.
* Smaller clusters are available.
* If there is a charm release in edge, it will be used.

Make sure to test all charm upgrades and work in progress stuff there. If
necessary the environment can be completely destroyed and redeployed, so
don't worry about messing it up. For that reason it's important to keep
automated deployments working and eliminate the need for post-deploy manual
hacks.

Testing WIP changes
^^^^^^^^^^^^^^^^^^^

The ``charm release`` command demonstrated above releases to the *stable*
channel by default. If you want to test a change in staging before it is
merged into the main branch, you can release into *edge* with ``charm release
--channel=edge ...``, and then use

.. code-block::

  $ terraform plan
  $ terraform apply

Under the staging user as usual to test your change. Staging tracks edge by
default.
