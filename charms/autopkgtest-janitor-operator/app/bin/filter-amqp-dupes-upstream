#!/usr/bin/python3
# Filter out all but the latest request for a given upstream PR

import argparse
import json
import logging
import os
from collections import defaultdict

import dateutil.parser
import pika


def filter_amqp(options):
    amqp_con = pika.BlockingConnection(
        parameters=pika.ConnectionParameters(
            host=os.environ["RABBIT_HOST"],
            credentials=pika.PlainCredentials(
                username=os.environ["RABBIT_USER"],
                password=os.environ["RABBIT_PASSWORD"],
            ),
        ),
    )
    dry_run = "[dry-run] " if options.dry_run else ""

    queues = (
        f"debci-upstream-{release}-{arch}"
        for release in os.environ["RELEASES"].split(" ")
        for arch in os.environ["ARCHES"].split(" ")
    )
    for queue_name in queues:
        ch = amqp_con.channel()
        logging.debug("Looking at %s", queue_name)
        seen = defaultdict(dict)
        while True:
            try:
                method, _, body = ch.basic_get(queue_name)
            except pika.exceptions.ChannelClosedByBroker:
                logging.debug(f"No such queue {queue_name}")
                break
            if body is None:
                break
            (pkg, params) = body.split("\n", 1)
            params_j = json.loads(params)
            submit_time = dateutil.parser.parse(params_j["submit-time"])
            pr = [
                val.split("=", 1)[1]
                for val in params_j["env"]
                if val.startswith("UPSTREAM_PULL_REQUEST")
            ][0]
            try:
                (delivery_tag, old_submit_time) = seen[pkg][pr]
                if old_submit_time <= submit_time:
                    # pylint: disable=line-too-long
                    logging.info(
                        f"{dry_run}We have seen PR {pr} in {queue_name} before: acking the previous request"
                    )
                    if not options.dry_run:
                        ch.basic_ack(
                            delivery_tag
                        )  # delivery tag, the old one NOT r.delivery_tag!
                del seen[pkg][pr]
            except KeyError:
                pass
            finally:
                logging.debug(f"Recording {pkg}/{pr} for {queue_name}")
                seen[pkg][pr] = (method.delivery_tag, submit_time)


def main():
    parser = argparse.ArgumentParser(
        description="""Deduplicates jobs in the upstream queue.

The upstream integration is different than regular jobs pushed by Britney.
If a developer pushes two times in a row on a pull request, then two test
requests get queued. This script is here to remove any duplicate requests.

The script expects rabbitmq credentials as well as target releases and arches
to be loaded via environment variables.
""",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="only show the operations that would be performed",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="additionally show queue items that are not removed",
    )

    args = parser.parse_args()

    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format="%(asctime)s - %(message)s",
    )

    filter_amqp(args)


if __name__ == "__main__":
    main()
